# Batch Jobs Overview
### Common Technologies Used:
* Python Scripts
    * Can be run in Kubernetes
* SQL
* Spark
* Flink

### Pros vs Cons
#### Advantages of Batch Jobs
* Easy to manage
* Easy to scale
* Retry ability
#### Disadvantage of Batch Jobs
* Delayed time to see

### Reality
In the industry, about 80% of enterprises use batch jobs rather than streaming. Only very specific use cases require streaming jobs.

# Spark Introduction
## What is Spark?
* Data Processing Engine
* Can be used for both batch and streaming jobs

## When to use Spark?
* Typically used when you have a datalake with objects which you would like to run SQL on, and write it back to the datalake.
* If you can express your batch job as SQL, consider using Hive or Presto/Athena
* Things that cannot be expressed with SQL, Spark may be a better solution

# Spark Installation on Linux

## First - Get Java installed
1. Install Java - go to `https://jdk.java.net/archive/`, and find version 11.0.1. Copy the link and `wget` it.
2. Unzip it by running `tar xzfv <filename>`. Remove the compressed file by running `rm <filename>`
3. Now create a variable using `export JAVA_HOME="${HOME}/spark/<jdk-filepath>/"` (note: if the $HOME variable does not point to the right filepath, you can manually write it)
4. Also run `PATH='${JAVA_HOME}/bin:${PATH}'`

    ```
    export JAVA_HOME="/workspaces/zoomcamp2024/spark/jdk-11.0.1/"
    export PATH="${JAVA_HOME}/bin:${PATH}"
    ```

5. Now, running `which java` should show you the filepath with your java installation, and running `java --version` should point you to the newest downloaded version.

## Second - Install Spark
1. Google `Download Spark`, should take you to the Apache Spark website. For this demo, we use Spark release `3.0.3`, package type `Pre-built for Apache Hadoop 3.2 and later`. wget the file. 

    ```
    wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz
    ```
2. Same as before, extract the file using `tar xzfv`:
    ```
    tar xzfv spark-3.0.3-bin-hadoop3.2.tgz 
    ```
3. Now declare the variables:
    ```
    export SPARK_HOME="/workspaces/zoomcamp2024/spark/spark-3.0.3-bin-hadoop3.2/"

    export PATH="${SPARK_HOME}/bin:${PATH}"
    ```
4. Start your spark environment: `spark-shell`
5. Run the following sample code to ensure that everything works:
    ```
    val data = 1 to 10000
    val distData = sc.parallelize(data)
    distData.filter(_ < 10).collect()
    ```

## Third - Storing the variables to bash
1. Since we don't want to re-type the variables everytime we re-enter, use `nano ~/.bashrc` to edit the bash startup file, and add all the previous export commands to the end of the file:
    ```
    export JAVA_HOME="/workspaces/zoomcamp2024/spark/jdk-11.0.1/"
    export PATH="${JAVA_HOME}/bin:${PATH}"
    export SPARK_HOME="/workspaces/zoomcamp2024/spark/spark-3.0.3-bin-hadoop3.2/"
    export PATH="${SPARK_HOME}/bin:${PATH}"
    ```
2. Run `Ctrl-O` to save, then `Ctrl-X` to exit.
3. Using `which java` and `which pyspark` should now point to the versions in your folder

# PySpark Setup
## Start Jupyter Notebook
1. Navigate to your notebooks folder and run `jupyter notebook`
2. Run the following:
    ```
    export PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
    export PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH"
    ```
3. In a new notebook, run `import pyspark`, then run `pyspark.__version__`. It should show `3.0.3`.
4. You can also use `pyspark.__file__` to show where the module is located.
5. Download the data file in the notebook:
   ``` 
    !wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv
   ```
6. You can now run a full PySpark test using the following:
    ```
    import pyspark
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .master("local[*]") \
        .appName('test') \
        .getOrCreate()

    df = spark.read \
        .option("header", "true") \
        .csv('taxi+_zone_lookup.csv')

    df.show()
    ```
7. Now let's save it to a parquet file:
   ```
   df.write.parquet('zones')
   ```