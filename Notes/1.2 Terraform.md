# Installation and Setup of Terraform Environment

## Setup of Terraform Environment
Create a new project, then create a service account in Google IAM, and assign it the appropriate roles, in this case, Storage, Compute and BigQuery Admin. Download the JSON keys and store them in your terraform workspace.

Create a file called `main.tf` in your Terraform directory, which is where we will execute going forward.

For Google Cloud, we can Google for "Terraform Google Provider" and copy the Provider code, as well as the config requirements. Here is a sample:

```
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "5.13.0"
    }
  }
}

provider "google" {
  project = "de2024-dylan"
  region  = "asia-southeast1"
}
```

To provide the credentials, we can use `credentials='path/to/credentials'` inside the provider setup, but to be safer, we can use an environment variable in Linux. 

In the command line, set the environment variable to be the path to the JSON key: `export GOOGLE_CREDENTIALS='path/to/credentials.json'`. You can test this by typing `$echo $GOOGLE_CREDENTIALS`. Terraform will automatically use this environment variable if available.

## Initializing Terraform Environment
With the `main.tf` file fully setup, you can now type `terraform init` to initialize your connection. If successfully, a new folder called `.terraform` should be created in the directory, with the required code.

## Setup of GCP Bucket from Terraform
Let's create a bucket in GCP using Terraform. Try searching Google for `terraform google cloud storage bucket`, and navigating to the HashiCorp sample. It may look something like this:

```
resource "google_storage_bucket" "demo-bucket" {
  name          = "de2024-dylan-bucket"
  location      = "ASIA-SOUTHEAST1"
  force_destroy = true

  lifecycle_rule {
    condition {
      age = 1
    }
    action {
      type = "AbortIncompleteMultipartUpload"
    }
  }
}
```

Note in the line where the resource is declared, there are two arguments, `google_storage_bucket` and `demo-bucket`. The first is to indicate the type of resource, while the second is a local name (Terraform name), so we can refer to this in the future using `resource_type.name`, e.g. `google_storage_bucket.demo-bucket`.

You can save the terraform file at this point and run `terraform plan`. This should give you an output of the changes that will take place. Review it, and if all is good, run it with `terraform apply`.

Notice that after running `terraform apply`, a state file is created (`terraform.tfstate`). This state file will provide you the metadata of the resources you just created.

## Tearing down infrastructure
Just run the command `terraform destroy`. Confirm and apply, and your infrastrucre should be removed.

## Warning: Github uploads
To prevent uploading of sensitive data to Github (such as your access keys), use a gitignore file. Search google for a .gitignore file for reference.

```
# Local .terraform directories
**/.terraform/*

# .tfstate files
*.tfstate
*.tfstate.*

# Crash log files
crash.log
crash.*.log

# Exclude all .tfvars files, which are likely to contain sensitive data, such as
# password, private keys, and other secrets. These should not be part of version 
# control as they are data points which are potentially sensitive and subject 
# to change depending on the environment.
*.tfvars
*.tfvars.json

# Ignore override files as they are usually used to override resources locally and so
# are not checked in
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Include override files you do wish to add to version control using negated pattern
# !example_override.tf

# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan
# example: *tfplan*

# Ignore CLI configuration files
.terraformrc
terraform.rc
```

## Terraform: Creating a BigQuery dataset
As usual, try searching Google for it. There should be an example code block to test. See sample:

```
resource "google_bigquery_dataset" "demo-dataset" {
  dataset_id    = "de2024_dylan_dataset"
  friendly_name = "de2024_dylan_dataset"
  description   = "Sample Dataset with Terraform"
  location      = "ASIA-SOUTHEAST1"
}
```
## Using variables in Terraform
Create a file called `variables.tf`. Declare variables in it using the following conventions:

```
variable "bq_dataset_name" {
    description = "My BigQuery Dataset Name"
    default = "de2024_dylan_dataset"
}

variable "gcs_bucket_name" {
    description = "My Storage Bucket Name"
    default = "de2024-dylan-bucket"
}

variable "gcs_storage_class" {
    description = "Bucket Storage Class"
    default = "STANDARD"
}
```

Now, in the `main.tf`, you can use your declared variables by calling them with `var.<variable name>`, for example, `var.bq_dataset_name`.

## Using file function in main.tf for credentials
We can stop using the `GOOGLE_CREDENTIALS` environment variable and declare our key filepath in the `variables.tf` using the following:

```
variable "credentials" {
  description = "Path to Credentials"
  default     = "./keys/gcp-keys.json"
}
```

Now, use the `var.credentials` in the provider code block in `main.tf`, and you can unset the previous environment variables using `unset GOOGLE_CREDENTIALS`.

The `main.tf` will now have this codeblock:
```
provider "google" {
  project     = var.project
  region      = var.location
  credentials = file(var.credentials)
}
```