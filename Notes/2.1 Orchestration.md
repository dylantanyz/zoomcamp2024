# Introduction to Orchestration
A large part of data engineering is ETL data between sources. Orchestration is a process of dependdency management, facilitated through automation. The data orchestrator manages scheduling, triggering, monitoring, resource allocation.

Every workflow requires sequential steps. Steps = Tasks (in Mage lingo), Workflows = DAGs (Directed Acrylic Graphs)

# Mage
## Installing Mage
We will use Docker to run Mage. Let's first clone the example repo:

```
git clone https://github.com/mage-ai/mage-zoomcamp.git mage-zoomcamp
```

There are some environment variables that will contain credentials later, so let's make a copy which will be ignored by git:

```
cp dev.env .env
```

Now, run `docker compose build`. (ensure you're in the mage-zoomcamp directory!)

(Note: To update Mage in the future, you can run `docker pull mageai/mageai:latest`)

## Starting and Connecting to Mage
Run `docker compose up`. (Note that this also runs a Postgres instance, because of the docker-compose that we pulled)

Once the logs are running, you can connect to Mage, by default using the port 6789, in the browser. (e.g. `localhost:6789`)

## Using a simple pipeline
Navigate to the left -> Click the "Pipelines" button. When entering the Pipeline, on the left, you can click "Edit Pipeline" to see the codeblocks.

Each block usually consists of a main function (e.g. `@data_loader`, `@transformer`) and a test funtion (`@test`).

You can execute and test each block individually, but also test the full pipeline by going to the last block, clicking the ellipses, and clicking "Execute with all upstream blocks".

## Configuring Postgres Connectivity
Navigate to the `io_config.yaml`, either in Mage or Vscode. Here is where the connectivity variables are stored. You can declare different environments, e.g. `dev`, by declaring it at the first-level indedntation on the file, for example:

```
dev:
  POSTGRES_CONNECT_TIMEOUT: 10
  POSTGRES_DBNAME: postgres
  POSTGRES_SCHEMA: public # Optional
  POSTGRES_USER: username
  POSTGRES_PASSWORD: password
  POSTGRES_HOST: hostname
  POSTGRES_PORT: 5432
```

You can also pass in the environment variables from the VM `.env` file which was in the docker compose file for the Mage service, by encapsulating in double curly brackets, then using `env_var()`, like so:

```
dev:
  POSTGRES_CONNECT_TIMEOUT: 10
  POSTGRES_DBNAME: {{ env_var('POSTGRES_DBNAME') }}
  POSTGRES_SCHEMA: {{ env_var('POSTGRES_SCHEMA') }}
  POSTGRES_USER: {{ env_var('POSTGRES_USER') }}
  POSTGRES_PASSWORD: {{ env_var('POSTGRES_PASSWORD') }}
  POSTGRES_HOST: {{ env_var('POSTGRES_HOST') }}
  POSTGRES_PORT: {{ env_var('POSTGRES_PORT') }}
```
Save it, and you can test your connection to Postgres by creating a new pipeline and creating a SQL Data Loader. Select `PostgreSQL` as the connection, and change the environment to `dev` to test it.

## Creating a simple pipeline
### Data Loading
Create a new pipeline, and then create use a Data Loader with Python > API.

We will load the data in this url: `https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz`

Then we will want to declare the datatypes which will optimize the amount of memory pandas uses when processing this CSV. You can declare a dictionary of data types, which you can pass in to the `read_csv` function later.

Declaring data types:
```
    taxi_dtypes = {
            'VendorID': pd.Int64Dtype(),
            'passenger_count': pd.Int64Dtype(),
            'trip_distance': float,
            'RatecodeID':pd.Int64Dtype(),
            'store_and_fwd_flag':str,
            'PULocationID':pd.Int64Dtype(),
            'DOLocationID':pd.Int64Dtype(),
            'payment_type': pd.Int64Dtype(),
            'fare_amount': float,
            'extra':float,
            'mta_tax':float,
            'tip_amount':float,
            'tolls_amount':float,
            'improvement_surcharge':float,
            'total_amount':float,
            'congestion_surcharge':float
    }
```
There are two datetime columns, which we can also pass along to pandas, by first declaring a list:
```
parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']
```

Putting it all together when reading the csv:
```
return pd.read_csv(url, sep=",", compression="gzip", dtype=taxi_dtypes, parse_dates=parse_dates)
```

### Data Transformation
Let's create a new Transformer block, selecting Generic.

The original dataset had some trips where the passenger count was 0, which we want to remove. We can first identify the number of rows that have 0 for passenger count:
```
print(f"Preprocessing: rows with zero passengers {data['passenger_count'].isin([0]).sum()}")
```

Now, let's only return the dataset with `passenger_count` greater than 0:

```
return data[data['passenger_count'] > 0]
```

Now let's create an assertion (a test) that ensures that there are no passenger counts == 0.

```
@test
def test_output(output, *args) -> None:
    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'
```

Putting it all together, we get this:

```
if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@transformer
def transform(data, *args, **kwargs):
    print(f"Preprocessing: rows with zero passengers {data['passenger_count'].isin([0]).sum()}")
    # Specify your transformation logic here

    # return data[data['passenger_count'] > 0]
    return data

@test
def test_output(output, *args) -> None:
    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'
```

### Data Export
Create a new Export block, selecting Python > Postgres. The variables within the @data_exporter block are pretty self explanatory. Note the `if_exists='replace'` argument, which is crucial for idempotent transactions. 

```
from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.postgres import Postgres
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
    schema_name = 'ny_taxi'  # Specify the name of the schema to export data to
    table_name = 'yellow_cab_data'  # Specify the name of the table to export data to
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'dev'

    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
        loader.export(
            df,
            schema_name,
            table_name,
            index=False,  # Specifies whether to include index in exported table
            if_exists='replace',  # Specify resolution policy if table name already exists
        )

```
To verify the data has been loaded, you can create another SQL data loader block, and run the query in raw SQL on the dev environment:

```
SELECT * FROM ny_taxi.yellow_cab_data LIMIT 10
```

# Writing to GCP
## GCP Setup
Create a standard GCS bucket. Create a new service account that will access this bucket (for simplicity, we granted this account owner permissions). Create and download the JSON service account key and put it in the Mage project folder.

Note that in the `docker-compose.yaml` in the Mage folder, the `.:/home/src/` path is mounted, meaning it will pull files from the mage project folder into the mage volume.

Enter Mage again, and use the Terminal (navbar on left). Doing a `ls`, we can see that the JSON credentials are now available. Navigate to Files and edit `io-config.yaml`, and put the path to the JSON file in the `GOOGLE_SERVICE_ACC_KEY_FILEPATH` parameter. Remember to delete the lines above which allow you to manually fill in the JSON contents.

Now, the BigQuery pipeline should be useable. You can try a pipeline SQL data loader and switch the source to BigQuery, run something like `SELECT 1;` to verify the connectivity. Also, you can try a GCS Bucket data loader, and change the `bucket_name` and `object_key` variables accordingly to what was setup in GCP earlier.

## Writing an ETL Pipeline to GCS
Create a new standard batch pipeline. We can reuse the same load API block from earlier by navigating to `data_loaders` and dragging the `load_api_data` over to the main window. 

Create a data loader and transformer by dragging previous scripts, and then create a new data explorer (GCS Python) using the name `taxi_to_gcs_parquet`. Set the `bucket_name` variable, and the `object_key` field should end with `.parquet`. Mage will infer the output requirements from the extension. Try running with all upstream blocks and the parquet file should be written into the GCS bucket.

## Writing into a partitioned parquet file on GCS
Create a new Generic Python data exporter, and name it `taxi_to_gcs_partitioned_parquet`. Connect it to the `transform_taxi_data` block instead of the previous data exporter, so it will run in parallel with the other code. 

First, let's do the necessary imports for this:
```
import pyarrow as pa
import pyarrow.parquet as pq
import os
```

Declare the path to the Google credentials:
```
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/home/src/yourcredentials.json"
```
Also declare your project id, bucket name and table name:
```
project_id = 'de2024-dylan'
bucket_name = 'zoomcamp-dylan'
table_name = 'nyc_taxi_data'
```

Finally declare a root_path:
```
root_path = f'{bucket_name}/{table_name}'
```

We initialize a date column which we want to partition on:
```
data['tpep_pickup_date'] = data['tpep_pickup_datetime'].dt.date
```

Let's also load the table into pyArrow and prepare the GCS filesystem:
```
table = pa.Table.from_pandas(data)
gcs = pa.fs.GcsFileSystem()
```

Finally, we put it all together by writing to GCS using PyArrow:
```
pq.write_to_dataset(
    table,
    root_path=root_path,
    partition_cols=['tpep_pickup_date'],
    filesystem=gcs
)
```

Running it will all upstream blocks, we should see the files written into GCS, partitioned into parquet files by dates. We can see how PyArrow has iterated away the chunking logic to partition the dataset.

## Writing from GCS into BigQuery
Create a new batch pipeline. Create a GCS data loader, load the Parquet file. Then create a Generic Python Data Transformer. We are going to standardize data names by replacing spaces with underscores and lowercasing all characters. 

```
    data.columns = (data.columns
                    .str.replace(' ', '_')
                    .str.lower()
    )
```

Now, create a SQL data exporter and select BigQuery as the connection. Note that the dataframe is available from the previous block. We can write it into bigquery by specifying the schema and tablename, then running:

```
SELECT * FROM {{ df_1 }}
```

Once the above code is run, you should be able to see the table with the data written into it in BigQuery.

## Scheduling Mage Pipelines (Simple)
In Mage, go to "Triggers" on the left navpanel. Create a new trigger, select "Schedule".. the rest is self explanatory.

## Parameterized Execution
Specifiying variables for the execution of the data pipeline. For this lesson, we will specify runtime variables to get a unique daily set of parameters (writing incremental payloads by date).

Clone the previously created pipeline that creates partitioned files, and delete the block that writes the partitions. Create a new python data loader and copy over the previous data exporter code. Remove the connection to the previous data exporter block, delete that too, and link your new `paramterized_exporter` block to the data transformer block.

Note that the main function has a `**kwargs` (key words) argument passed into it. You can call upon it in the function, for example, calling the current datetime through ther following:

```
now = kwargs.get('execution_date')
print(now) # Entire timestamp
print(now.date()) # Just the date
```

To print it into a filepath friendly format, we can use something like this: `print(now.strftime('%Y/%m/%d'))`.

We can then insert it into the main code as an object_key.

```
    now = kwargs.get('execution_date')
    now_fpath = now.strftime('%Y/%m/%d')

    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    bucket_name = 'zoomcamp-dylan'
    object_key = f'{now_fpath}/nyc_taxi_data.parquet'
    print(object_key)

    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(
        df,
        bucket_name,
        object_key,
    )
```

Now, running the pipeline, the file should be written into GCS in the corresponding date filepath. You can also define custom variables for other runs.

## Backfills
In a typical system, to backfill or replicate previously missing/lost data, you would typically have to use custom scripts. In Mage, you can easily backfill by going into a pipeline and setting backfill in the left navbar. This is useful only for date-related backfills for now.

