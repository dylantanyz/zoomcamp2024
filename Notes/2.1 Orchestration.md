# Introduction to Orchestration
A large part of data engineering is ETL data between sources. Orchestration is a process of dependdency management, facilitated through automation. The data orchestrator manages scheduling, triggering, monitoring, resource allocation.

Every workflow requires sequential steps. Steps = Tasks (in Mage lingo), Workflows = DAGs (Directed Acrylic Graphs)

# Mage
## Installing Mage
We will use Docker to run Mage. Let's first clone the example repo:

```
git clone https://github.com/mage-ai/mage-zoomcamp.git mage-zoomcamp
```

There are some environment variables that will contain credentials later, so let's make a copy which will be ignored by git:

```
cp dev.env .env
```

Now, run `docker compose build`. (ensure you're in the mage-zoomcamp directory!)

(Note: To update Mage in the future, you can run `docker pull mageai/mageai:latest`)

## Starting and Connecting to Mage
Run `docker compose up`. (Note that this also runs a Postgres instance, because of the docker-compose that we pulled)

Once the logs are running, you can connect to Mage, by default using the port 6789, in the browser. (e.g. `localhost:6789`)

## Using a simple pipeline
Navigate to the left -> Click the "Pipelines" button. When entering the Pipeline, on the left, you can click "Edit Pipeline" to see the codeblocks.

Each block usually consists of a main function (e.g. `@data_loader`, `@transformer`) and a test funtion (`@test`).

You can execute and test each block individually, but also test the full pipeline by going to the last block, clicking the ellipses, and clicking "Execute with all upstream blocks".

## Configuring Postgres Connectivity
Navigate to the `io_config.yaml`, either in Mage or Vscode. Here is where the connectivity variables are stored. You can declare different environments, e.g. `dev`, by declaring it at the first-level indedntation on the file, for example:

```
dev:
  POSTGRES_CONNECT_TIMEOUT: 10
  POSTGRES_DBNAME: postgres
  POSTGRES_SCHEMA: public # Optional
  POSTGRES_USER: username
  POSTGRES_PASSWORD: password
  POSTGRES_HOST: hostname
  POSTGRES_PORT: 5432
```

You can also pass in the environment variables from the VM `.env` file which was in the docker compose file for the Mage service, by encapsulating in double curly brackets, then using `env_var()`, like so:

```
dev:
  POSTGRES_CONNECT_TIMEOUT: 10
  POSTGRES_DBNAME: {{ env_var('POSTGRES_DBNAME') }}
  POSTGRES_SCHEMA: {{ env_var('POSTGRES_SCHEMA') }}
  POSTGRES_USER: {{ env_var('POSTGRES_USER') }}
  POSTGRES_PASSWORD: {{ env_var('POSTGRES_PASSWORD') }}
  POSTGRES_HOST: {{ env_var('POSTGRES_HOST') }}
  POSTGRES_PORT: {{ env_var('POSTGRES_PORT') }}
```
Save it, and you can test your connection to Postgres by creating a new pipeline and creating a SQL Data Loader. Select `PostgreSQL` as the connection, and change the environment to `dev` to test it.

## Creating a simple pipeline
### Data Loading
Create a new pipeline, and then create use a Data Loader with Python > API.

We will load the data in this url: `https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz`

Then we will want to declare the datatypes which will optimize the amount of memory pandas uses when processing this CSV. You can declare a dictionary of data types, which you can pass in to the `read_csv` function later.

Declaring data types:
```
    taxi_dtypes = {
            'VendorID': pd.Int64Dtype(),
            'passenger_count': pd.Int64Dtype(),
            'trip_distance': float,
            'RatecodeID':pd.Int64Dtype(),
            'store_and_fwd_flag':str,
            'PULocationID':pd.Int64Dtype(),
            'DOLocationID':pd.Int64Dtype(),
            'payment_type': pd.Int64Dtype(),
            'fare_amount': float,
            'extra':float,
            'mta_tax':float,
            'tip_amount':float,
            'tolls_amount':float,
            'improvement_surcharge':float,
            'total_amount':float,
            'congestion_surcharge':float
    }
```
There are two datetime columns, which we can also pass along to pandas, by first declaring a list:
```
parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']
```

Putting it all together when reading the csv:
```
return pd.read_csv(url, sep=",", compression="gzip", dtype=taxi_dtypes, parse_dates=parse_dates)
```

### Data Transformation
Let's create a new Transformer block, selecting Generic.

The original dataset had some trips where the passenger count was 0, which we want to remove. We can first identify the number of rows that have 0 for passenger count:
```
print(f"Preprocessing: rows with zero passengers {data['passenger_count'].isin([0]).sum()}")
```

Now, let's only return the dataset with `passenger_count` greater than 0:

```
return data[data['passenger_count'] > 0]
```

Now let's create an assertion (a test) that ensures that there are no passenger counts == 0.

```
@test
def test_output(output, *args) -> None:
    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'
```

Putting it all together, we get this:

```
if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@transformer
def transform(data, *args, **kwargs):
    print(f"Preprocessing: rows with zero passengers {data['passenger_count'].isin([0]).sum()}")
    # Specify your transformation logic here

    # return data[data['passenger_count'] > 0]
    return data

@test
def test_output(output, *args) -> None:
    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'
```

### Data Export
Create a new Export block, selecting Python > Postgres. The variables within the @data_exporter block are pretty self explanatory. Note the `if_exists='replace'` argument, which is crucial for idempotent transactions. 

```
from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.postgres import Postgres
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
    schema_name = 'ny_taxi'  # Specify the name of the schema to export data to
    table_name = 'yellow_cab_data'  # Specify the name of the table to export data to
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'dev'

    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
        loader.export(
            df,
            schema_name,
            table_name,
            index=False,  # Specifies whether to include index in exported table
            if_exists='replace',  # Specify resolution policy if table name already exists
        )

```
To verify the data has been loaded, you can create another SQL data loader block, and run the query in raw SQL on the dev environment:

```
SELECT * FROM ny_taxi.yellow_cab_data LIMIT 10
```