# Environment Setup
* Loaded NYTaxi Green and Yellow 2019-2020 data into BigQuery
* Created a dbt account and project
* Created a dataset within BigQuery
* Linked the project to BigQuery via a Service Account

# Concepts
## Roles in a Data Team
* Data Engineer: Prepares and maintain the infrastructure the data team needs
* Data Analyst: Uses data to answer questions and solve problems
* Analytics Engineer: Fills the gap between both - Introduces good software engineering practices to the efforts of data analysts and scientists

## Tools used
* Data Loading:
* Data Storing: Cloud data warehouses like Snowflake, Bigquery, Redshift
* Data Modelling: Tools like dbt or Dataform
* Data Presentation: BI tools like Looker, Tableau, PowerBI

## Data Modelling
### ETL vs ELT
ETL has slightly more stable and compliant data anallysis. Higher storage and compute costs.

ELT is faster and more flexible data analysis. Lower cost and lower maintenance. 

## Dimensional Modeling
### Objective
Deliver data understandable to the business users, and delivery fast query performance

### Approach
Prioritize user understandability and query performance over non redundant data (3NF)

### Elements of Dimensional Modeling
#### Facts Table
* Measurements, metrics or facts
* Corresponds to a business process
* "verbs"

#### Dimensions Tables
* Corresponds to a business entity
* Provides context to a business process
* "nouns"

# DBT Project
## Starting on BigQuery
After creating the BigQuery connection and Github repo connection on your project, go to the IDE and initialize your dbt project.

Create a branch called `dbt-project`. In the file explorer, open up `dbt_project.yml`. Edit under "models:" and add `taxi_rides_ny:` in an indented line.

## DBT Models
DBT models are .sql scripts. The model has four materialization strategies: 
* Table: Drops and recreates the table
* View: Virtual table
* Incremental: Update existing tables
* Ephemeral: Only exist during the duration of a dbt run

## DBT Sources (FROM)
Sources can be configured in the .yaml file. You can define the name, database and schema, in one location, then call it from all the models using it using the source argument e.g. `FROM {{ source('staging','table_name') }}`. Source freshness can be defined and tested (threshold of age of data can be defined). 

Seeds are another type of source that can be used, which are CSV files stored in the repository, which gets the benefits of version control. Useful for lookup values (tables which don't change as much).

## Create a model
Under the models folder, create a 'staging' folder, then create a 'schema.yml' file in it. You can define the schemas accordingly here. Note that 'database' is equivalent to 'dataset' in BigQuery terms.

Sample:
```
version: 2

sources:
  - name: staging
    database: de2024-dylan
    schema: trips_data_all
    tables:
      - name: green_tripdata
      - name: yellow_tripdata
```

Note that when you define tables names, the Cloud IDE will prompt with a 'Generate Model' button that automatically suggests DDL. This will generate a .sql file that has a view of that table.

Delete the 'example' model, and run `dbt build` to ensure that it is working correctly.

## Macros
DBT allows you to combine SQL with Jinja, a templating language. Macros allow you to:

* Use control structures (e.g. if statements and for loops) in SQL
* Use env variables in dbt projects
* Operate on the results for one query to generate another query
* Abstract snippets of SQL into reuseable macros (similar to functions in other programming languages)

Let's create a macro to return the payment type description. Go to the macros folder, and create a file called `get_payment_type_description.sql`. Here's the code we're using:

```
{#
    This macro returns the description of the payment_type 
#}

{% macro get_payment_type_description(payment_type) -%}

    case {{ dbt.safe_cast("payment_type", api.Column.translate_type("integer")) }}  
        when 1 then 'Credit card'
        when 2 then 'Cash'
        when 3 then 'No charge'
        when 4 then 'Dispute'
        when 5 then 'Unknown'
        when 6 then 'Voided trip'
        else 'EMPTY'
    end

{%- endmacro %}
```

Now, we can call upon this macro in our model by invoking it using: `{{ get_payment_type_description(payment_type) }} as payment_type_descripted`. For example, in the `stg_green_tripdata.sql` file, we can insert it as a column in our DDL (as part of the select statement).

After saving the macro and the model file, try running 'Compile' and see how dbt parses the model code, returning the macro into the DDL.

## Packages in dbt
Packages are like libraries in other programming languages. They are standalone dbt projects. When adding a package to your project, all macros and models become available. It is imported via the `packages.yml` file. 

Create a `packages.yml` in the root of the project folder. Use the following:

```
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
```

Then run `dbt deps` to get the package into the project.

Now, we will use the `generate_surrogate_key()` function from the `dbt-utils` package in the `stg_green_tripdata` model. Open up the model and add the following to the start of the select statement:

```
select
  {{ dbt_utils.generate_surrogate_key(['vendorid', 'lpep_pickup_datetime']) }} as tripid,
  vendorid,
  lpep_pickup_datetime,
  etc
```
Try compiling the above model and see how it parses the function. You should also be able to build it without issue. 

From this point, copy and paste the repository code for `stg_green_tripdata.sql`, and we will work from there. (Extra hint for dbt: typing `__` (double underscores) in the dbt IDE will allow you to quickly fill-in code blocks, e.g. config code blocks.)

## Variables
Similar to programming variables, allows to define values that can be used across the project. With a macro, dbt allows us to provide data to models for compilation. To use a variable, we can call the `var('...')` function. Variables can be defined in two ways:

* In the `dbt_project.yml` file
* On the command line

In the sample code, we see it being called at the end of the script:
```
{% if var('is_test_run', default=true) %}

  limit 100

{% endif %}
```
In this sample, `is_test_run` is defaulted to true, limiting the results to 100. If we pass in a different variable, it will no longer be true.

However, if we run it via the CLI but pass in false, it will not run the `limit 100` clause:
```
dbt build --select stg_green_tripdata --vars '{'is_test_run':'false'}'
```
See the details in the build and note that there is no limit this time. (Hint: This piece of code is useful, called a devlimit, as it makes for cheaper queries during development)

Now, let's continue with creating a new `stg_yellow_tripdata.sql` in the models folder, and pasting the repo code.

## Creating the core model, and using seeds
Import the `taxi_zone_lookup.csv` into the `seeds` folder (or, create the file, and copy-paste the contents).

Create a new `core` folder in the models folder. Then create `dim_zones.sql` file inside that folder, and copy the content. This will reference the seed file created earlier.

Now create a new file, `fact_trips.sql`, which will capture all the green and yellow trip data in one file. You can copy and paste the code, but essentially it is doing the following:

1. Referencing and selecting from green and yellow staging data tables, but adding a `service_type` column to identify the taxi type
2. Creating a union of both tables data, which already shares the same structure
3. Joining them on the `dim_zones` model
   
Now, we can build the entire project. Click Build > Upstream and Downstream. You should also add the variable to indicate this is not a test run so that the full dataset is loaded. 

## DBT Tests
Tests are assumptions we make about our data. They are essentially a `select` sql query that get compiled and returns the number of failing records. They are defined on a column in a .yml file. 

DBT's provides some basic tests out of the box:
* Unique
* Not null
* Accepted values
* Foreign key to another table

Custom tests can also be created. Create and build the `dm_monthly_zone_revenue.sql` file (refer to repo). Also, update the `packages.yml` to get codegen:
```
- package: dbt-labs/codegen
  version: 0.12.1
```

Let's use the `generate_model_yaml` function, with the following code snippet:

```
{% set models_to_generate = codegen.get_models(directory='staging', prefix='stg') %}
{{ codegen.generate_model_yaml(
    model_names = models_to_generate
) }}
```

Compile the code, and you can copy and paste the output into the `schema.yml`. Now, look for the declared `tripid` column and add the following test for uniqueness:

```
- name: tripid
  data_type: string
  description: ""
  tests:
    - unique:
        severity: warn
    - not_null:
        severity: warn
```

Also, add the following warning for `pickup_locationid`:
```
* code here *
  tests:
    - relationships:
        field: locationid
        to: ref('taxi_zone_lookup')
        severity: warn
```

For the `payment_type` field, we want to make sure to be informed on any new values added. We can use:

```
tests:
  - accepted_values:
      values:
        - (1,2,3,4,5)
```
Alternatively, we can add these values as a project variable in the `dbt-project.yml`:
```
vars:
  payment_type_values: [1, 2, 3, 4, 5]
```
So now, the new schema snippet is:
```
tests:
  - accepted_values:
      values: "{{ var('payment_type_values') }}"
      severity: warn
      quote: false
```
The `quote: false` is a statement necessary for BigQuery specifically. It should not be necessary for Postgres.

Now, let's run `dbt build` and run all the tests. It should provide a warning for the `accepted_values` test.

## DBT Documentation
DBT provides a way to generate documentation for your dbt project and render it as a website. It creates information about your project (model code, dependencies, sources, DAG, descriptions) and information about the data warehouse (column names and types, table stats like size and rows)

To use it, run `dbt docs generate`. After running, click the book icon in the top of the left navbar to view the docs site. 

## Deployment of DBT Project
Deployment runs the models we created in development in the production environment. A deployment workflow will look something like this:

* Develop in a user branch
* Open a PR to merge into the main branch
* Merge the branch into main branch
* Run the new models in the production environment using the main branch
* Schedule the models

Create a new environment and call it `Production`. Then create a Deploy job. Note that the default command is `dbt build`. You can also set the trigger, and we'll use a time trigger for now. Remember that the latest code should be merged into the main branch for your code to be successful.

### Continuous Integration (CI)
CI is the practice of regularly merging development branches into a central repository, after which automated builds and tests are run. The goal is to reduce adding bugs to code and maintain a more stable project.

dbt allows CI on pull requests. You can create a CI job that gets triggered on pull requests.
