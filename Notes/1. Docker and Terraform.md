# Installation and Setup of Postgres Docker Image

## Setup of Docker and Postgres

### Building the docker image
Run this in the command line. The last part is 'name':'version' (e.g. test:pandas)
```
docker build -t postgres:13
```

### Running the docker image
The '-it' argument is a combination of '-i', which is '--interactive', which keeps the standard input open, and '-t', or '-tty', which simulates a terminal.
```
docker run -it postgres:13
```

## Sample Postgres dockerfile
Note the "volumes" section, which specifies where in the persistent filesystem the postgres DB should be mounted to, so the Postgres instance and the actual records are decoupled. We will specify the environment variables and mounted volumes using command line arguments in the next section.
```
services:
    postgres:
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: always
```
# Running image and connecting
## Running the Postgres docker image with environment variables
When running a docker file, you can specify: 
<ul>
    <li>Declare environment variables with the "-e" argument.</li>
    <li>Map volume to host machine:VM with the "-v" arugment.</li>
    <li>Map ports from host machine:VM with "-p" argument.</li>
</ul>

```
docker run -it \
    -e POSTGRES_USER="root" \
    -e POSTGRES_PASSWORD="root" \
    -e POSTGRES_DB="ny_taxi" \
    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \
    -p 5432:5432 \
    postgres:13
```

## Connecting to the Postgres instance using pgcli
Once Postgres docker image is running, open a new terminal and run pgcli (may have to install using `pip install pgcli`)
```
pgcli -h localhost -p 5432 -u root -d ny_taxi
```

## Downloading sample data to environment
To download sample data, use `wget <URL>` to download sample files as required. Sample data used is [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/yellow)

If file is gzipped, you can use `gunzip <filepath>` to unzip it.

If your sample data is too large, you can just get a subset of the first X rows using the following command: (drop the part after '100' if you want to view only without writing to a new file)
```
head -n 100 yellow_tripdata_2019-01.csv > yellow_tripdata_head.csv
```

# Loading CSV into Postgres
## Using pandas to get DDL
You can use pandas to load a dataframe into your Postgres server. Remember to make sure that the Postgres server is running before doing this.

In the meantime, load up jupyter - In the console, type `jupyter notebook`. 

The code below loads the dataframe into memory and prints the DDL.

```
import pandas as pd
df = pd.read_csv('filename.csv', nrows=100)
print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))
```
## Converting text to datetime
If pandas does not recognize timestamps, you can use the function `pd.to_datetime(df.columname)` to parse them accordingly.

```
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
```
## Create connection to Postgres
Pandas uses a library called SQLAlchemy to connect to Postgres. You can use `pip install sqlalchemy psycopg2` to get it.

The code below allows you to connect to the postgres server:
```
from sqlalchemy import create_engine
engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi)
engine.connect()
```

## Writing to table
If there is an extremely large file to write, you can use iterators to chunk the writing to the table.

```
df = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=10000)
```

You can then load the iterations using `df = next(df_iter)`. Here's the sample code that iterates to writing into the table:
```
from time import time
while True:
    t_start = time()
    
    df = next(df_iter)
    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')

    t_end = time()
    
    print("Inserted another chunk, took %.3f" %(t_end - t_start))
```

## Confirming details of table in pgcli
To explore the table using pgcli, connect to the pgcli console (see above), then you can list tables with `\dt`, or describe the table using `\d <tablename>`. Press `q` to exit from viewing table details.

To verify the rows have been successfully inserted, you can use `SELECT COUNT(1) FROM <TABLENAME>`


# Managing Postgres with pgAdmin
## Installing pgAdmin on docker
You can run pgAdmin on docker using the following image:
```
docker run -it \
    -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" \
    -e PGADMIN_DEFAULT_PASSWORD="root" \
    -p 8080:80 \
    dpage/pgadmin4
```

Since it is mapped to port 8080, you can navigate to it by going to `localhost:8080` on the browser. However, you will not be able to establish a connection to the Postgres server as they are running on two separate containers (e.g. two different "localhosts")

## Creating a docker network
To link between pgAdmin and Postgres, let's create a docker network

```
docker network create [OPTIONS] NETWORK
```

For example, let's run `docker network create pg-network`, then re-run the docker containers with the following two lines added at the bottom of each run (the `network` and `name`)
```
docker run -it \
    -e POSTGRES_USER="root" \
    -e POSTGRES_PASSWORD="root" \
    -e POSTGRES_DB="ny_taxi" \
    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \
    -p 5432:5432 \
    --network=pg-network \
    --name pg-database \
    postgres:13

docker run -it \
    -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" \
    -e PGADMIN_DEFAULT_PASSWORD="root" \
    -p 8080:80 \
    --network=pg-network \
    --name pgadmin \
    dpage/pgadmin4
```

## Configuring pgAdmin
Now, we can connect in pgAdmin. Go to `localhost:8080`, login, register a new server, and use `pg-database` as the Host name, and enter the user and pass. You should now be able to view your Postgres instance in pgAdmin.

# Dockerizing the ingestion script
## Converting jupyter notebook to a python script
To convert, use the following command:
```
jupyter nbconvert --to=script upload-data.ipynb
```
Clean up the converted file and move the lines around as needed. To pass along the environment variables, we can use Python's built-in `argparse` module. Create the `main` function in the script and pass in the required arguments. Refer to the python script for full details.

## Dockerizing the python script
To download the file within the Docker container, you have to install the wget package. Sample dockerfile for this build:

```
FROM python:3.9

RUN apt-get install wget
RUN pip install pandas sqlalchemy psycopg2

WORKDIR /app
COPY ingest_data.py ingest_data.py

ENTRYPOINT ["python", "ingest_data.py"]
```

Then, build the dockerfile.(See the first section for a refresher)

## (OPTIONAL) Running a local server to download the CSV
You can wget from a localhost webserver by running the following command:
```
python -m http.server --bind 0.0.0.0 8000
```

However, as your docker container is an isolated network, you have to specify the host IP via DNS. By default on Linux environments, the docker host IP is `172.17.0.1`. For Windows and Mac, you can point to `host.docker.internal`.

## Running the ingestion dockerfile
See sample below:
```
URL='http://172.17.0.1:8000/yellow_tripdata_2019-01.csv'
docker run -it \
    --network=pg-network \
    taxi_ingest:v004 \
    --user=root \
    --password=root \
    --host=pg-database \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_trips \
    --url=${URL}
```

# Docker Compose
## Creating a docker compose file
Create a new file in the root folder with the name `docker-compose.yaml`. In it, declare two services: `pg-database` and `pgadmin`. You will declare the images, environment variables, ports and volumes where necessary. See sample below:

```
services:
  pgdatabase:
    image: postgres:13
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=ny_taxi
    volumes:
      - ./ny_taxi_postgres_data:/var/lib/postgresql/data:rw
    ports:
      - "5432:5432"
  pgadmin:
    image: dpage/pgadmin4
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@admin.com 
      - PGADMIN_DEFAULT_PASSWORD=root
    ports:
      - "8080:80"
```

You can run the compose file using the command `docker-compose up`, and add `-d` in the argument to run it in detached mode. After it spins up, you can use the images as previous. To shut down the services, use `docker-compose down`.

# SQL Refresher
## JOINS
To view the taxi trip pickup/dropoff locations in actual names instead of reference codes, we can use the taxi zone lookup table, specifiying AND conditions where the values match the lookup table.

```
SELECT 
 tpep_pickup_datetime,
 tpep_dropoff_datetime,
 total_amount,
 CONCAT(zpu."Borough", ' / ',zpu."Zone") AS "pickup_loc",
 CONCAT(zdo."Borough",' / ',zdo."Zone") AS "dropoff_loc"
FROM 
			 yellow_taxi_trips t,
	 zones zpu,
 zones zdo
WHERE
 t."PULocationID" = zpu."LocationID" AND
 t."DOLocationID" = zdo."LocationID"
LIMIT 100;

```
or using the JOIN command:
```
SELECT 
 tpep_pickup_datetime,
 tpep_dropoff_datetime,
 total_amount,
 CONCAT(zpu."Borough", ' / ',zpu."Zone") AS "pickup_loc",
 CONCAT(zdo."Borough",' / ',zdo."Zone") AS "dropoff_loc"
FROM 
	yellow_taxi_trips t 
    JOIN zones zpu
    ON  t."PULocationID" = zpu."LocationID"
    JOIN zones zdo
    ON  t."DOLocationID" = zdo."LocationID"
LIMIT 100;
```

Remember the types of joins:
<ul>
<li>INNER JOIN: Only returns rows where values are present on both tables</li>
<li>LEFT/RIGHT JOIN: Returns ALL values on the left/right speciifed table, regardless if there is a match or not</li>
<li>OUTER JOIN: Returns ALL rows from BOTH tables
</ul>

## GROUP BYs
For grouping results by the column, and aggregating results. We want to find out the number of trips for each day of the month. We also want the results to be returned in order.

```
SELECT 
    CAST(tpep_dropoff_datetime AS DATE) as "day",
    COUNT(1)
FROM 
	yellow_taxi_trips t 
GROUP BY
    CAST(tpep_dropoff_datetime AS DATE)
ORDER BY
    "day" ASC;
```

You can also group by SELECT statements using numbers, where the first SELECT = 1, second is 2, etc. See below:

```
SELECT 
    CAST(tpep_dropoff_datetime AS DATE) as "day",
    "DOLocationID",
    COUNT(1) as "count",
    MAX(total_amount),
    MAX(passenger_count)
FROM 
	yellow_taxi_trips t 
GROUP BY
    1,2 
ORDER BY
    "day" ASC,
    "DOLocationID" ASC;
```