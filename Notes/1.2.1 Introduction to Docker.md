# Installation and Setup of Postgres Docker Image

## Setup of Docker and Postgres

### Building the docker image
Run this in the command line. The last part is 'name':'version' (e.g. test:pandas)
```
docker build -t postgres:13
```

### Running the docker image
The '-it' argument is a combination of '-i', which is '--interactive', which keeps the standard input open, and '-t', or '-tty', which simulates a terminal.
```
docker run -it postgres:13
```

## Sample Postgres dockerfile
Note the "volumes" section, which specifies where in the persistent filesystem the postgres DB should be mounted to, so the Postgres instance and the actual records are decoupled. We will specify the environment variables and mounted volumes using command line arguments in the next section.
```
services:
    postgres:
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: always
```
# Running image and connecting
## Running the Postgres docker image with environment variables
When running a docker file, you can specify: 
<ul>
    <li>Declare environment variables with the "-e" argument.</li>
    <li>Map volume to host machine:VM with the "-v" arugment.</li>
    <li>Map ports from host machine:VM with "-p" argument.</li>
</ul>

```
docker run -it \
    -e POSTGRES_USER="root" \
    -e POSTGRES_PASSWORD="root" \
    -e POSTGRES_DB="ny_taxi" \
    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \
    -p 5432:5432 \
    postgres:13
```

## Connecting to the Postgres instance using pgcli
Once Postgres docker image is running, open a new terminal and run pgcli (may have to install using `pip install pgcli`)
```
pgcli -h localhost -p 5432 -u root -d ny_taxi
```

## Downloading sample data to environment
To download sample data, use wget \<filename\> to download sample files as required. Sample data used is [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/yellow).

If your sample data is too large, you can just get a subset of the first X rows using the following command: (drop the part after '100' if you want to view only without writing to a new file)
```
head -n 100 yellow_tripdata_2019-01.csv > yellow_tripdata_head.csv
```

# Loading CSV into Postgres
## Using pandas to get DDL
You can use pandas to load a dataframe into your Postgres server. Remember to make sure that the Postgres server is running before doing this.

In the meantime, load up jupyter - In the console, type `jupyter notebook`. 

The code below loads the dataframe into memory and prints the DDL.

```
import pandas as pd
df = pd.read_csv('filename.csv', nrows=100)
print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))
```
## Converting text to datetime
If pandas does not recognize timestamps, you can use the function `pd.to_datetime(df.columname)` to parse them accordingly.

```
df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
```
## Create connection to Postgres
Pandas uses a library called SQLAlchemy to connect to Postgres. You can use `pip install sqlalchemy psycopg2` to get it.

The code below allows you to connect to the postgres server:
```
from sqlalchemy import create_engine
engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi)
engine.connect()
```

## Writing to table
If there is an extremely large file to write, you can use iterators to chunk the writing to the table.

```
df = pd.read_csv('yellow_tripdata_2021-01.csv', iterator=True, chunksize=10000)
```

You can then load the iterations using `df = next(df_iter)`. 